{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import string, re, json\n",
    "\n",
    "# Import NLTK\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://192.168.1.27/TwitterSentimentAnalysis.Covid19?retryWrites=true\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://192.168.1.27/TwitterSentimentAnalysis.Covid19?retryWrites=true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "#Download the needed corpus \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- full_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_noRetweet = \"[\\\n",
    "    {\\\n",
    "        '$match': {\\\n",
    "            'lang': 'en',\\\n",
    "            'retweeted_status':null\\\n",
    "        }\\\n",
    "    },{\\\n",
    "        '$project': {\\\n",
    "            'id_str': 1\\\n",
    "            'created_at': 1\\\n",
    "            'full_text': 1\\\n",
    "        },\\\n",
    "    }\\\n",
    "]\"\n",
    "\n",
    "pipeline_Retweet = \"[\\\n",
    "    {\\\n",
    "        '$match': {\\\n",
    "            'lang': 'en'\\\n",
    "            'retweeted_status':{$ne: null}\\\n",
    "            'retweeted_status.lang': 'en'\\\n",
    "        }\\\n",
    "    },{\\\n",
    "        '$project': {\\\n",
    "            'id_str': 1\\\n",
    "            'created_at': 1\\\n",
    "            'retweeted_status.full_text': 1\\\n",
    "        },\\\n",
    "    }\\\n",
    "]\"\n",
    "\n",
    "df_ENGNoRetweet = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline_noRetweet).load()\n",
    "df_ENGRetweet = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline_Retweet).load()\n",
    "\n",
    "df_ENGNoRetweet.printSchema()\n",
    "df_ENGRetweet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19113\n",
      "59851\n"
     ]
    }
   ],
   "source": [
    "print (df_ENGNoRetweet.count())\n",
    "print (df_ENGRetweet.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43710"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unione dei risultati\n",
    "df_Tweets = df_ENGRetweet\\\n",
    "    .selectExpr(\"id_str\", \"retweeted_status.full_text as full_text\")\\\n",
    "    .union(df_ENGNoRetweet.select(\"id_str\", \"full_text\"))\n",
    "\n",
    "df_Tweets = df_Tweets.select(\"full_text\").distinct()\n",
    "df_Tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "def remove_all_space(astring):\n",
    "  return \" \".join(astring.split())\n",
    "\n",
    "# clean the text \n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    cleaned_str2 = remove_all_space(cleaned_str)\n",
    "    return cleaned_str2\n",
    "\n",
    "\n",
    "# extract part of speech\n",
    "def pos(tokenized_text):\n",
    "    sent_tag_list = pos_tag(tokenized_text) \n",
    "    aList = []\n",
    "    for word, tag in sent_tag_list:\n",
    "        tagToUse = ''\n",
    "        if tag.startswith('J'):\n",
    "            tagToUse= 'a' # aggettivi\n",
    "        elif tag.startswith('N'):\n",
    "            tagToUse= 'n' # sostantivi\n",
    "        elif tag.startswith('R'):\n",
    "            tagToUse= 'r' # avverbi\n",
    "        elif tag.startswith('V'):\n",
    "            tagToUse= 'v' # verbi\n",
    "        else:\n",
    "            continue\n",
    "        aList.append((word, tagToUse))\n",
    "    return aList\n",
    "\n",
    "# lemmatize the commit comments  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(array_of_word_for_a_comment):\n",
    "    all_words_in_comment = []\n",
    "    for word in array_of_word_for_a_comment:\n",
    "        lemma = lemmatizer.lemmatize(word[0], pos=word[1])\n",
    "        if not lemma:\n",
    "            continue\n",
    "        all_words_in_comment.append([lemma,word[1]])  \n",
    "    return all_words_in_comment\n",
    "\n",
    "\n",
    "#calculate the sentiment \n",
    "def cal_score(array_of_lemma_tag_for_a_comment):\n",
    "    alist = [array_of_lemma_tag_for_a_comment]\n",
    "    totalScore = 0\n",
    "    count_words_included = 0\n",
    "    for word in array_of_lemma_tag_for_a_comment:\n",
    "        synset_forms = list(swn.senti_synsets(word[0], word[1]))\n",
    "        if not synset_forms:\n",
    "            continue\n",
    "        synset = synset_forms[0] \n",
    "        totalScore = totalScore + synset.pos_score() - synset.neg_score()\n",
    "        count_words_included = count_words_included +1\n",
    "    final_dec = ''\n",
    "    if count_words_included == 0:\n",
    "        final_dec = 'N/A'\n",
    "    elif totalScore == 0:\n",
    "        final_dec = 'Neu'        \n",
    "    elif totalScore/count_words_included < 0:\n",
    "        final_dec = 'Neg'\n",
    "    elif totalScore/count_words_included > 0:\n",
    "        final_dec = 'Pos'\n",
    "    return final_dec\n",
    "\n",
    "\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "pos_udf = udf(pos,ArrayType(StructType([ StructField(\"word\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "lemmatize_udf = udf(lemmatize,ArrayType(StructType([ StructField(\"lemma\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "cal_score_udf = udf(cal_score,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                      cleaned_text|\n",
      "+--------------------------------------------------+\n",
      "|the united states long accustomed thinking itse...|\n",
      "|coronavirus amp climate change demand similar r...|\n",
      "|baby first memory watches parents lose their jo...|\n",
      "|from uprising outbreak hong kong sign language ...|\n",
      "|another little way can help our neighbors covid...|\n",
      "|country facing medical and economic crisis the ...|\n",
      "|anthony fauci the idea anybody getting test eas...|\n",
      "|chinese number lie there are many more than thi...|\n",
      "|union ministry health and family welfare total ...|\n",
      "|mhc amp its partners are actively monitoring th...|\n",
      "|the government has just declared state emergenc...|\n",
      "|kumailn will join you shouting from the rooftop...|\n",
      "|new poll italy hints the possible political eff...|\n",
      "|the medical professionals first responders groc...|\n",
      "|breaking gov pritzker ordering all bars and res...|\n",
      "|together can defeat corona not let panic creep ...|\n",
      "|short the trump pressured emergency fed move va...|\n",
      "|breaking air and army national guard profession...|\n",
      "|minutes democraticdebate biden wants everything...|\n",
      "|ecuador had the covid southamerica now diseased...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove noise\n",
    "df_TweetsCleaned = df_Tweets.withColumn(\"cleaned_text\", remove_features_udf(df_Tweets['full_text']))\n",
    "df_TweetsCleaned.select('cleaned_text').show(truncate=50)\n",
    "#df_TweetsCleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           full_text|        cleaned_text|\n",
      "+--------------------+--------------------+\n",
      "|\"The United State...|the united states...|\n",
      "|Coronavirus &amp;...|coronavirus amp c...|\n",
      "|Oh to be a 1998 b...|baby first memory...|\n",
      "|From uprising to ...|from uprising out...|\n",
      "|Another little wa...|another little wa...|\n",
      "|“Our country is f...|country facing me...|\n",
      "|😷 Dr. Anthony Fa...|anthony fauci the...|\n",
      "|Chinese number is...|chinese number li...|\n",
      "|Union Ministry of...|union ministry he...|\n",
      "|MHC &amp; its par...|mhc amp its partn...|\n",
      "|As the government...|the government ha...|\n",
      "|@kumailn 📣I will...|kumailn will join...|\n",
      "|New poll in Italy...|new poll italy hi...|\n",
      "|To the medical pr...|the medical profe...|\n",
      "|BREAKING: Gov. JB...|breaking gov prit...|\n",
      "|Together we can d...|together can defe...|\n",
      "|In short, the Tru...|short the trump p...|\n",
      "|BREAKING: \n",
      "\n",
      "“Abou...|breaking air and ...|\n",
      "|1st 10 minutes of...|minutes democrati...|\n",
      "|Ecuador had the 1...|ecuador had the c...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizzazione\n",
    "tkn = Tokenizer()\\\n",
    "      .setInputCol(\"cleaned_text\")\\\n",
    "      .setOutputCol(\"words\")\n",
    "\n",
    "# Eliminazione Stopwords\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "        .setStopWords(englishStopWords)\\\n",
    "        .setInputCol(\"words\")\\\n",
    "        .setOutputCol(\"words_nsw\")\n",
    "\n",
    "pipeline = Pipeline(stages = [tkn, stops])\n",
    "\n",
    "\n",
    "df_TweetsCleaned = pipeline\\\n",
    "    .fit(df_TweetsCleaned.select(\"full_text\", \"cleaned_text\"))\\\n",
    "    .transform(df_TweetsCleaned.select(\"full_text\", \"cleaned_text\"))\n",
    "\n",
    "df_TweetsCleaned.select(\"full_text\", \"cleaned_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                     words_nsw_tag|\n",
      "+--------------------------------------------------+\n",
      "|[[united, a], [states, n], [long, r], [accustom...|\n",
      "|[[coronavirus, n], [amp, n], [climate, n], [cha...|\n",
      "|[[baby, n], [first, r], [memory, n], [watches, ...|\n",
      "|[[uprising, a], [outbreak, n], [hong, n], [kong...|\n",
      "|[[little, a], [way, n], [help, n], [neighbors, ...|\n",
      "|[[country, n], [facing, v], [medical, a], [econ...|\n",
      "|[[anthony, n], [fauci, n], [idea, n], [anybody,...|\n",
      "|[[chinese, a], [number, n], [lie, v], [many, a]...|\n",
      "|[[union, n], [ministry, n], [health, n], [famil...|\n",
      "|[[mhc, n], [amp, a], [partners, n], [actively, ...|\n",
      "|[[government, n], [declared, v], [state, n], [e...|\n",
      "|[[kumailn, n], [join, n], [shouting, v], [rooft...|\n",
      "|[[new, a], [poll, n], [italy, a], [hints, n], [...|\n",
      "|[[medical, a], [professionals, n], [first, a], ...|\n",
      "|[[breaking, v], [gov, n], [pritzker, n], [order...|\n",
      "|[[together, r], [defeat, n], [corona, a], [let,...|\n",
      "|[[short, a], [trump, n], [pressured, v], [emerg...|\n",
      "|[[breaking, v], [air, n], [army, a], [national,...|\n",
      "|[[minutes, n], [democraticdebate, v], [biden, a...|\n",
      "|[[ecuador, n], [covid, n], [southamerica, n], [...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tag for part of speech\n",
    "df_TweetsCleanedTagged = df_TweetsCleaned.withColumn(\"words_nsw_tag\", pos_udf(df_TweetsCleaned['words_nsw']))\n",
    "df_TweetsCleanedTagged.select(\"words_nsw_tag\").show(truncate=50)\n",
    "#df_TweetsCleanedTagged.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                  words_lemmatized|\n",
      "+--------------------------------------------------+\n",
      "|[[united, a], [state, n], [long, r], [accustom,...|\n",
      "|[[coronavirus, n], [amp, n], [climate, n], [cha...|\n",
      "|[[baby, n], [first, r], [memory, n], [watch, n]...|\n",
      "|[[uprising, a], [outbreak, n], [hong, n], [kong...|\n",
      "|[[little, a], [way, n], [help, n], [neighbor, n...|\n",
      "|[[country, n], [face, v], [medical, a], [econom...|\n",
      "|[[anthony, n], [fauci, n], [idea, n], [anybody,...|\n",
      "|[[chinese, a], [number, n], [lie, v], [many, a]...|\n",
      "|[[union, n], [ministry, n], [health, n], [famil...|\n",
      "|[[mhc, n], [amp, a], [partner, n], [actively, r...|\n",
      "|[[government, n], [declare, v], [state, n], [em...|\n",
      "|[[kumailn, n], [join, n], [shout, v], [rooftop,...|\n",
      "|[[new, a], [poll, n], [italy, a], [hint, n], [p...|\n",
      "|[[medical, a], [professional, n], [first, a], [...|\n",
      "|[[break, v], [gov, n], [pritzker, n], [order, v...|\n",
      "|[[together, r], [defeat, n], [corona, a], [let,...|\n",
      "|[[short, a], [trump, n], [pressure, v], [emerge...|\n",
      "|[[break, v], [air, n], [army, a], [national, a]...|\n",
      "|[[minute, n], [democraticdebate, v], [biden, a]...|\n",
      "|[[ecuador, n], [covid, n], [southamerica, n], [...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lemmatize the tokens \n",
    "df_Tweet_Lemmatized = df_TweetsCleanedTagged.withColumn(\"words_lemmatized\", lemmatize_udf(df_TweetsCleanedTagged['words_nsw_tag']))\n",
    "df_Tweet_Lemmatized.select(\"words_lemmatized\").show(truncate=50)\n",
    "#df_Tweet_Lemmatized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----+\n",
      "|                                         full_text|score|\n",
      "+--------------------------------------------------+-----+\n",
      "|\"The United States, long accustomed to thinking...|  Pos|\n",
      "|Coronavirus &amp; climate change demand similar...|  Pos|\n",
      "|Oh to be a 1998 baby\n",
      "\n",
      "✔️ first memory is 9/11\n",
      "\n",
      "...|  Neg|\n",
      "|From uprising to outbreak: Hong Kong sign langu...|  Neu|\n",
      "|Another little way we can help our neighbors. #...|  Pos|\n",
      "|“Our country is facing a medical and economic c...|  Neg|\n",
      "|😷 Dr. Anthony Fauci: \"The idea of anybody gett...|  Pos|\n",
      "|Chinese number is a lie. There are many more th...|  Neg|\n",
      "|Union Ministry of Health and Family Welfare: A ...|  Pos|\n",
      "|MHC &amp; its partners are actively monitoring ...|  Pos|\n",
      "|As the government has just declared state of em...|  Neg|\n",
      "|@kumailn 📣I will join you in shouting from the...|  Pos|\n",
      "|New poll in Italy hints at the possible politic...|  Pos|\n",
      "|To the medical professionals, first responders,...|  Pos|\n",
      "|BREAKING: Gov. JB Pritzker is ordering all bars...|  Neu|\n",
      "|Together we can defeat #corona if we do not let...|  Neu|\n",
      "|In short, the Trump pressured emergency Fed mov...|  Neg|\n",
      "|BREAKING: \n",
      "\n",
      "“About 400 Air and Army National Gu...|  Pos|\n",
      "|1st 10 minutes of #DemocraticDebate \n",
      "\n",
      "Biden wan...|  Pos|\n",
      "|Ecuador had the 1st #CoVID19 in SouthAmerica, n...|  Pos|\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate the sentiment\n",
    "try:\n",
    "    df_Tweet_Lemmatized = df_Tweet_Lemmatized.withColumn(\"score\", cal_score_udf(df_Tweet_Lemmatized[\"words_lemmatized\"]))\n",
    "except:#\n",
    "    sys.setrecursionlimit(2000)\n",
    "    df_Tweet_Lemmatized = df_Tweet_Lemmatized.withColumn(\"score\", cal_score_udf(df_Tweet_Lemmatized[\"words_lemmatized\"]))\n",
    "\n",
    "df_Tweet_Lemmatized.select(\"full_text\",\"score\").show(truncate=50)\n",
    "#df_Tweet_Lemmatized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43710"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Tweet_Lemmatized.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
