{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import string, re, json\n",
    "\n",
    "# Import NLTK\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://192.168.1.27/SentimentAnalysisSpark.Covid19?retryWrites=true\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://192.168.1.27/SentimentAnalysisSpark.LabeledTweetSentimentAnalysis?retryWrites=true\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "#Download the needed corpus \n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- full_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline_noRetweet = \"[\\\n",
    "    {\\\n",
    "        '$match': {\\\n",
    "            'lang': 'en',\\\n",
    "            'retweeted_status':null\\\n",
    "        }\\\n",
    "    },{\\\n",
    "        '$project': {\\\n",
    "            'id_str': 1\\\n",
    "            'created_at': 1\\\n",
    "            'full_text': 1\\\n",
    "        },\\\n",
    "    }\\\n",
    "]\"\n",
    "\n",
    "pipeline_Retweet = \"[\\\n",
    "    {\\\n",
    "        '$match': {\\\n",
    "            'lang': 'en'\\\n",
    "            'retweeted_status':{$ne: null}\\\n",
    "            'retweeted_status.lang': 'en'\\\n",
    "        }\\\n",
    "    },{\\\n",
    "        '$project': {\\\n",
    "            'id_str': 1\\\n",
    "            'created_at': 1\\\n",
    "            'retweeted_status.full_text': 1\\\n",
    "        },\\\n",
    "    }\\\n",
    "]\"\n",
    "\n",
    "df_ENGNoRetweet = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline_noRetweet).load()\n",
    "df_ENGRetweet = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline_Retweet).load()\n",
    "\n",
    "df_ENGNoRetweet.printSchema()\n",
    "df_ENGRetweet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_ENGNoRetweet.count())\n",
    "print (df_ENGRetweet.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50210"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unione dei risultati\n",
    "df_Tweets = df_ENGRetweet\\\n",
    "    .selectExpr(\"id_str\", \"retweeted_status.full_text as full_text\", \"created_at\")\\\n",
    "    .union(df_ENGNoRetweet.select(\"id_str\", \"full_text\", \"created_at\"))\n",
    "\n",
    "df_Tweets_noDup = df_Tweets.dropDuplicates([\"full_text\"])\n",
    "df_Tweets_noDup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "def remove_all_space(astring):\n",
    "  return \" \".join(astring.split())\n",
    "\n",
    "# clean the text \n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    cleaned_str2 = remove_all_space(cleaned_str)\n",
    "    return cleaned_str2\n",
    "\n",
    "\n",
    "# extract part of speech\n",
    "def pos(tokenized_text):\n",
    "    sent_tag_list = pos_tag(tokenized_text) \n",
    "    aList = []\n",
    "    for word, tag in sent_tag_list:\n",
    "        tagToUse = ''\n",
    "        if tag.startswith('J'):\n",
    "            tagToUse= 'a' # aggettivi\n",
    "        elif tag.startswith('N'):\n",
    "            tagToUse= 'n' # sostantivi\n",
    "        elif tag.startswith('R'):\n",
    "            tagToUse= 'r' # avverbi\n",
    "        elif tag.startswith('V'):\n",
    "            tagToUse= 'v' # verbi\n",
    "        else:\n",
    "            continue\n",
    "        aList.append((word, tagToUse))\n",
    "    return aList\n",
    "\n",
    "# lemmatize the commit comments  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(array_of_word_for_a_comment):\n",
    "    all_words_in_comment = []\n",
    "    for word in array_of_word_for_a_comment:\n",
    "        lemma = lemmatizer.lemmatize(word[0], pos=word[1])\n",
    "        if not lemma:\n",
    "            continue\n",
    "        all_words_in_comment.append([lemma,word[1]])  \n",
    "    return all_words_in_comment\n",
    "\n",
    "\n",
    "#calculate the sentiment \n",
    "def cal_score(array_of_lemma_tag_for_a_comment):\n",
    "    alist = [array_of_lemma_tag_for_a_comment]\n",
    "    totalScore = 0\n",
    "    count_words_included = 0\n",
    "    for word in array_of_lemma_tag_for_a_comment:\n",
    "        synset_forms = list(swn.senti_synsets(word[0], word[1]))\n",
    "        if not synset_forms:\n",
    "            continue\n",
    "        synset = synset_forms[0] \n",
    "        totalScore = totalScore + synset.pos_score() - synset.neg_score()\n",
    "        count_words_included = count_words_included +1\n",
    "    #final_dec = ''\n",
    "    if count_words_included == 0:\n",
    "        return 3\n",
    "    elif totalScore == 0:\n",
    "        return 0        \n",
    "    elif totalScore/count_words_included < 0:\n",
    "        return 2\n",
    "    elif totalScore/count_words_included > 0:\n",
    "        return 1\n",
    "\n",
    "\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "pos_udf = udf(pos,ArrayType(StructType([ StructField(\"word\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "lemmatize_udf = udf(lemmatize,ArrayType(StructType([ StructField(\"lemma\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "cal_score_udf = udf(cal_score,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+\n",
      "|                                      cleaned_text|\n",
      "+--------------------------------------------------+\n",
      "|from the beginning have said was matter when no...|\n",
      "|            this not what president trump said all|\n",
      "|recent health events across the globe surroundi...|\n",
      "|trump slump real and not being driven the coron...|\n",
      "|heck yes president trump taking care business a...|\n",
      "|new york confirms first coronavirus case govern...|\n",
      "|like ewarren for hhs crisis think she would exc...|\n",
      "|coronavirus causing mass hysteria but millions ...|\n",
      "|eight people have confirmed cases the novel cor...|\n",
      "|                                     told you plan|\n",
      "|covid been given small window opportunity manag...|\n",
      "|all top rok italy iran have coronavirus cases b...|\n",
      "|been shouting only tacha can save nigeria but a...|\n",
      "|joshngkamstra gail carson quote from the articl...|\n",
      "|breaking manila city mayor iskomoreno has order...|\n",
      "|now that one your constituents dead these lulz ...|\n",
      "|this year international women day comes crucial...|\n",
      "|the new york stock exchange halted stock tradin...|\n",
      "|those who think socialism wonderful the coronav...|\n",
      "|fife offers evidence support his assertions non...|\n",
      "+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove noise\n",
    "df_TweetsCleaned = df_Tweets_noDup.withColumn(\"cleaned_text\", remove_features_udf(df_Tweets_noDup['full_text']))\n",
    "df_TweetsCleaned.select('cleaned_text').show(truncate=50)\n",
    "#df_TweetsCleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizzazione\n",
    "tkn = Tokenizer()\\\n",
    "      .setInputCol(\"cleaned_text\")\\\n",
    "      .setOutputCol(\"words\")\n",
    "\n",
    "# Eliminazione Stopwords\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "        .setStopWords(englishStopWords)\\\n",
    "        .setInputCol(\"words\")\\\n",
    "        .setOutputCol(\"words_nsw\")\n",
    "\n",
    "pipeline = Pipeline(stages = [tkn, stops])\n",
    "\n",
    "\n",
    "df_TweetsCleaned = pipeline\\\n",
    "    .fit(df_TweetsCleaned.select(\"full_text\", \"cleaned_text\", \"created_at\"))\\\n",
    "    .transform(df_TweetsCleaned.select(\"full_text\", \"cleaned_text\", \"created_at\"))\n",
    "\n",
    "#df_TweetsCleaned.select(\"full_text\", \"cleaned_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag for part of speech\n",
    "df_TweetsCleanedTagged = df_TweetsCleaned.withColumn(\"words_nsw_tag\", pos_udf(df_TweetsCleaned['words_nsw']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize the tokens \n",
    "df_Tweet_Lemmatized = df_TweetsCleanedTagged.withColumn(\"words_lemmatized\", lemmatize_udf(df_TweetsCleanedTagged['words_nsw_tag']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+----------+----------+-------------+----------------+-----+\n",
      "| full_text|cleaned_text|created_at|     words| words_nsw|words_nsw_tag|words_lemmatized|score|\n",
      "+----------+------------+----------+----------+----------+-------------+----------------+-----+\n",
      "|\"First ...|  first p...|Mon Mar...|[first,...|[first,...|   [[first...|      [[first...|    1|\n",
      "|\"I feel...|  feel li...|Mon Apr...|[feel, ...|[feel, ...|   [[feel,...|      [[feel,...|    0|\n",
      "|\"Most a...|  most ap...|Mon Apr...|[most, ...|[applic...|   [[appli...|      [[appli...|    1|\n",
      "|\"The Un...|  the uni...|Mon Mar...|[the, u...|[united...|   [[unite...|      [[unite...|    1|\n",
      "|\"To lov...|  love pu...|Mon Mar...|[love, ...|[love, ...|   [[love,...|      [[love,...|    1|\n",
      "|#29. Mr...|  mrs amp...|Mon Mar...|[mrs, a...|[mrs, a...|   [[mrs, ...|      [[mr, n...|    1|\n",
      "|#BS \n",
      "#p...|  plainan...|Mon Apr...|[plaina...|[plaina...|   [[plain...|      [[plain...|    1|\n",
      "|#COVID1...|  covid h...|Mon Mar...|[covid,...|[covid,...|   [[covid...|      [[covid...|    2|\n",
      "|#Corona...|  coronav...|Mon Mar...|[corona...|[corona...|   [[coron...|      [[coron...|    0|\n",
      "|#Health...|  health ...|Mon Apr...|[health...|[health...|   [[healt...|      [[healt...|    1|\n",
      "|#Hopkin...|  hopkins...|Mon Apr...|[hopkin...|[hopkin...|   [[hopki...|      [[hopki...|    0|\n",
      "|#Italyâ€™...|  doctors...|Mon Mar...|[doctor...|[doctor...|   [[docto...|      [[docto...|    1|\n",
      "|#Kerala...|  kerala ...|Mon Apr...|[kerala...|[kerala...|   [[keral...|      [[keral...|    1|\n",
      "|#MoronT...|  morontr...|Mon Apr...|[moront...|[moront...|   [[moron...|      [[moron...|    1|\n",
      "|#Qanon ...|  qanon c...|Mon Mar...|[qanon,...|[qanon,...|   [[qanon...|      [[qanon...|    2|\n",
      "|#RT @is...|  islamin...|Mon Mar...|[islami...|[islami...|   [[love,...|      [[love,...|    0|\n",
      "|#Waco M...|  waco ma...|Mon Mar...|[waco, ...|[waco, ...|   [[waco,...|      [[waco,...|    2|\n",
      "|.@NYGov...|  nygovcu...|Mon Mar...|[nygovc...|[nygovc...|   [[nygov...|      [[nygov...|    2|\n",
      "|.@NYGov...|  nygovcu...|Mon Mar...|[nygovc...|[nygovc...|   [[nygov...|      [[nygov...|    1|\n",
      "|1. It t...|  took th...|Mon Apr...|[took, ...|[took, ...|   [[took,...|      [[take,...|    0|\n",
      "+----------+------------+----------+----------+----------+-------------+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate the sentiment\n",
    "\n",
    "try:\n",
    "    df_Tweet_Lemmatized_Score = df_Tweet_Lemmatized.withColumn(\"score\", cal_score_udf(df_Tweet_Lemmatized[\"words_lemmatized\"]))\n",
    "except:#\n",
    "    sys.setrecursionlimit(2000)\n",
    "    df_Tweet_Lemmatized_Score = df_Tweet_Lemmatized.withColumn(\"score\", cal_score_udf(df_Tweet_Lemmatized[\"words_lemmatized\"]))\n",
    "\n",
    "\n",
    "#df_Tweet_Lemmatized_Score = df_Tweet_Lemmatized.withColumn(\"score\", cal_score_udf(df_Tweet_Lemmatized[\"words_lemmatized\"]))\n",
    "\n",
    "df_Tweet_Lemmatized_Score.show(truncate=10)\n",
    "#df_Tweet_Lemmatized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50210"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Tweet_Lemmatized_Score.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
