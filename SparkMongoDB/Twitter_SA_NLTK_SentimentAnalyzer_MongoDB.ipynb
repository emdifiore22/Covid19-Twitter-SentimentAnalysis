{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import string, re, json\n",
    "\n",
    "# Import NLTK\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# Import numpy per \n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://192.168.1.27/SentimentAnalysisSpark.Covid19?retryWrites=true\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://192.168.1.27/SentimentAnalysisSpark.LabeledTweetsUTC?retryWrites=true\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_noRetweet = \"[\\\n",
    "    {\\\n",
    "        '$match': {\\\n",
    "            'lang': 'en',\\\n",
    "            'retweeted_status':null\\\n",
    "        }\\\n",
    "    },{\\\n",
    "        '$project': {\\\n",
    "            'id_str': 1\\\n",
    "            'created_at': 1\\\n",
    "            'full_text': 1\\\n",
    "        },\\\n",
    "    }\\\n",
    "]\"\n",
    "\n",
    "pipeline_Retweet = \"[\\\n",
    "    {\\\n",
    "        '$match': {\\\n",
    "            'lang': 'en'\\\n",
    "            'retweeted_status':{$ne: null}\\\n",
    "            'retweeted_status.lang':'en'\\\n",
    "        }\\\n",
    "    },{\\\n",
    "        '$project': {\\\n",
    "            'id_str': 1\\\n",
    "            'created_at': 1\\\n",
    "            'retweeted_status.full_text': 1\\\n",
    "        },\\\n",
    "    }\\\n",
    "]\"\n",
    "\n",
    "df_ENGNoRetweet = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline_noRetweet).load()\n",
    "df_ENGRetweet = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline_Retweet).load()\n",
    "\n",
    "#df_ENGNoRetweet.printSchema()\n",
    "#df_ENGRetweet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50210"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Tweets = df_ENGRetweet\\\n",
    "    .selectExpr(\"id_str\", \"retweeted_status.full_text as full_text\", \"created_at\")\\\n",
    "    .union(df_ENGNoRetweet.select(\"id_str\", \"full_text\", \"created_at\"))\n",
    "\n",
    "df_Tweets_noDup = df_Tweets.dropDuplicates([\"full_text\"])\n",
    "df_Tweets_noDup.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Tweets_noDup.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderSentimentAnalysis(data_str):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(data_str)\n",
    "    ss.pop('compound', None)\n",
    "    maximum = max(ss, key=ss.get)  \n",
    "    if maximum == 'neu':\n",
    "        if(ss['neu'] >= 0.6):\n",
    "            return 0\n",
    "        elif(ss['pos'] > ss['neg']):\n",
    "            return 1\n",
    "        elif(ss['neg'] > ss['pos']):\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "    elif maximum == 'pos':\n",
    "        return 1\n",
    "    elif maximum == 'neg':\n",
    "        return 2\n",
    "    \n",
    "vaderSentimentAnalysis_udf = udf(vaderSentimentAnalysis, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tweets_noDup_Labeled = df_Tweets_noDup.withColumn(\"label\", vaderSentimentAnalysis_udf(df_Tweets_noDup['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+-----+\n",
      "|             id_str|           full_text|          created_at|label|\n",
      "+-------------------+--------------------+--------------------+-----+\n",
      "|1234294581631442944|\"First positive c...|Mon Mar 02 01:48:...|    0|\n",
      "|1254892996904071170|\"I feel like I do...|Mon Apr 27 21:59:...|    0|\n",
      "|1254692729319235584|\"Most applicants ...|Mon Apr 27 08:43:...|    0|\n",
      "|1239564950198259712|\"The United State...|Mon Mar 16 14:51:...|    0|\n",
      "|1239569884100669440|\"To love purely i...|Mon Mar 16 15:11:...|    0|\n",
      "|1244744395817172998|#29. Mrs &amp; Mr...|Mon Mar 30 21:52:...|    0|\n",
      "|1254584662602723328|#BS \n",
      "#plainandsim...|Mon Apr 27 01:34:...|    0|\n",
      "|1242123497310208010|#COVID19 HMG have...|Mon Mar 23 16:18:...|    0|\n",
      "|1242131756918149123|#Coronavirus: #Fl...|Mon Mar 23 16:51:...|    0|\n",
      "|1252294117494849536|#Health #KY - Fli...|Mon Apr 20 17:52:...|    0|\n",
      "|1252284339204886532|#HopkinsEngineer ...|Mon Apr 20 17:13:...|    0|\n",
      "|1242130472672485376|#Italyâ€™s doctors ...|Mon Mar 23 16:45:...|    0|\n",
      "|1249554597145804802|#Kerala has #Flat...|Mon Apr 13 04:26:...|    0|\n",
      "|1249734800945991680|#MoronTrump \n",
      "\n",
      "RT ...|Mon Apr 13 16:22:...|    0|\n",
      "|1237024658681192449|#Qanon if China r...|Mon Mar 09 14:37:...|    0|\n",
      "|1242131014610083840|#RT @islaminind: ...|Mon Mar 23 16:48:...|    0|\n",
      "|1242120209726689283|#Waco Mayor Kyle ...|Mon Mar 23 16:05:...|    0|\n",
      "|1239573845327183874|.@NYGovCuomo givi...|Mon Mar 16 15:26:...|    0|\n",
      "|1239578492020224007|.@NYGovCuomo talk...|Mon Mar 16 15:45:...|    0|\n",
      "|1254908867802169359|1. It took the Ne...|Mon Apr 27 23:02:...|    0|\n",
      "+-------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Tweets_noDup_Labeled.where(\"label = 0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "from pyspark.sql.functions import to_date, to_utc_timestamp\n",
    "\n",
    "## Converting date string format\n",
    "def getDate(x):\n",
    "    if x is not None:\n",
    "        return str(datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y').replace(tzinfo=pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "## UDF declaration\n",
    "date_fn = udf(getDate, StringType())\n",
    "\n",
    "## Converting datatype in spark dataframe\n",
    "df_Tweets_noDup_LabeledUTC = df_Tweets_noDup_Labeled.withColumn(\"created_at_UTC\", to_utc_timestamp(date_fn(\"created_at\"),\"UTC\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tweets_noDup_LabeledUTC.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- created_at_UTC: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_Tweets_noDup_LabeledUTC.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
