{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import string, re, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           full_text|\n",
      "+--------------------+\n",
      "|CORONAVIRUS IN CA...|\n",
      "|Torta sfornata\n",
      "Pe...|\n",
      "|🇪🇺 Il Parlament...|\n",
      "|In un #AnfieldRoa...|\n",
      "|Cosa si può e non...|\n",
      "|Sono 2.162 le per...|\n",
      "|@carmelitadurso P...|\n",
      "|MOTUS-E ha scelto...|\n",
      "|Il teatro vive, a...|\n",
      "|#ParigiConsiglia ...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Caricamento dati necessari\n",
    "#\n",
    "#\n",
    "\n",
    "tweets = \"./iorestoacasa_1_original.json\"\n",
    "# JSON contenente una lista di strutture. Ogni struttura contiene:\n",
    "# - Parola\n",
    "# - positive_Score\n",
    "# - negativeScore\n",
    "sentix = \"./sentix.json\"\n",
    "\n",
    "# Creazione dataFrame\n",
    "df_Tweets = spark.read.format(\"json\").option(\"inferSchema\", \"true\").option(\"multiLine\", \"true\").load(tweets)\n",
    "df_Tweets.select(\"extended_tweet.full_text\").show(10)\n",
    "\n",
    "# Creazione JSON Sentix per l'etichettatura\n",
    "f = open(sentix)\n",
    "sentix_words = json.load(f) \n",
    "#print(sentix_words[0]['lemma'])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione funzione per l'eliminazione dei caratteri speciali\n",
    "#\n",
    "#\n",
    "\n",
    "def remove_punct(text):\n",
    "    \n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    special = re.compile('[\\$#,@&%£!=°§*/;-]')\n",
    "    num_re = re.compile('( \\\\d+)')\n",
    "\n",
    "    text = url_re.sub(\"\", text)\n",
    "    text = punc_re.sub(\"\", text)\n",
    "    text = mention_re.sub(\"\", text)\n",
    "    text = special.sub(\"\", text)\n",
    "    text = num_re.sub(\"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# setup pyspark udf function\n",
    "remove_features_udf = udf(lambda x: remove_punct(x), StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINIZIONE FUNZIONE PER L'ETICHETTATURA DEI TWEETS\n",
    "#\n",
    "#\n",
    "\n",
    "emoticonsPositive = ('😇','😊','❤️','😘','💞','💖','🤗','💕','👏','🎉','👍','🔝')\n",
    "emoticonsNegative = ('😂','😡','😠','😭','🤦‍','🤷🏼‍','😞','😱','😓','👎', '🇪🇺')\n",
    "\n",
    "def labeling(tweet):\n",
    "    val = 0\n",
    "    for word in tweet:\n",
    "        if (word in emoticonsPositive):\n",
    "            val = val + 1\n",
    "        elif (word in emoticonsNegative):\n",
    "            val = val - 1\n",
    "        else:\n",
    "            js = list(filter(lambda js: js['lemma']==word, sentix_words))\n",
    "            if(len(js)>0):\n",
    "                val = val + float(js[0]['positive_score'])\n",
    "                val = val - float(js[0]['negativeScore'])\n",
    "                \n",
    "    if(val>0):\n",
    "        # Positivo\n",
    "        return \"2\"\n",
    "    elif(val<0):\n",
    "        # Negativo\n",
    "        return \"1\"\n",
    "    else:\n",
    "        # Neutro\n",
    "        return \"0\"\n",
    "    \n",
    "# setup pyspark udf function\n",
    "label = udf(lambda x: labeling(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           words_nsw|\n",
      "+--------------------+\n",
      "|[coronavirus, cal...|\n",
      "|[torta, sfornata,...|\n",
      "|[🇪🇺, parlamento...|\n",
      "|[anfieldroad, inc...|\n",
      "|[cosa, può, può, ...|\n",
      "|[persone, state, ...|\n",
      "|[carmelitadurso, ...|\n",
      "|[motuse, scelto, ...|\n",
      "|[teatro, vive, si...|\n",
      "|[parigiconsiglia,...|\n",
      "+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminazione caratteri speciali\n",
    "df_noHash = df_Tweets.withColumn('words_filtered',remove_features_udf(\"extended_tweet.full_text\"))\n",
    "\n",
    "# Tokenizzazione\n",
    "tkn = Tokenizer()\\\n",
    "      .setInputCol(\"words_filtered\")\\\n",
    "      .setOutputCol(\"words\")\n",
    "\n",
    "# Eliminazione Stopwords\n",
    "italianStopWords = StopWordsRemover.loadDefaultStopWords(\"italian\")\n",
    "stops = StopWordsRemover()\\\n",
    "        .setStopWords(italianStopWords)\\\n",
    "        .setInputCol(\"words\")\\\n",
    "        .setOutputCol(\"words_nsw\")\n",
    "\n",
    "pipeline = Pipeline(stages = [tkn, stops])\n",
    "\n",
    "df_TweetCleaned = pipeline.fit(df_noHash.select(\"words_filtered\")).transform(df_noHash.select(\"words_filtered\"))\n",
    "\n",
    "df_TweetCleaned.select(\"words_nsw\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "code_folding": [
     4
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[words_filtered: string, words: array<string>, words_nsw: array<string>, label: string]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TweetLabeled = df_TweetCleaned.withColumn(\"label\", label(\"words_nsw\"))\n",
    "# df_TweetLabeled.show(5)\n",
    "df_TweetLabeled.persist()\n",
    "#print(df_TweetLabeled.count())\n",
    "#df_TweetLabeled.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           words_nsw|label|\n",
      "+--------------------+-----+\n",
      "|[coronavirus, cal...|    2|\n",
      "|[torta, sfornata,...|    1|\n",
      "|[🇪🇺, parlamento...|    1|\n",
      "|[anfieldroad, inc...|    1|\n",
      "|[cosa, può, può, ...|    2|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|           words_nsw|label|         rawFeatures|            features|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|[coronavirus, cal...|    2|(262144,[261,4772...|(262144,[261,4772...|[-178.21655521785...|[1.11957656329673...|       2.0|\n",
      "|[torta, sfornata,...|    1|(262144,[261,1940...|(262144,[261,1940...|[-243.71470617081...|[1.35380722355126...|       1.0|\n",
      "|[🇪🇺, parlamento...|    1|(262144,[261,904,...|(262144,[261,904,...|[-416.06166977245...|[3.60117116012188...|       1.0|\n",
      "|[anfieldroad, inc...|    1|(262144,[261,6880...|(262144,[261,6880...|[-243.04344706976...|[2.49446627738348...|       1.0|\n",
      "|[cosa, può, può, ...|    2|(262144,[261,3067...|(262144,[261,3067...|[-768.33841727772...|[1.26095400269135...|       2.0|\n",
      "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7825901439989156"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creazione DataFrame per il training\n",
    "df_TweetLabeled_toFit = df_TweetLabeled.select(\"words_nsw\", \"label\")\n",
    "df_TweetLabeled_toFit.show(5)\n",
    "\n",
    "# Estrazione delle features\n",
    "hashingTF = HashingTF(inputCol=\"words_nsw\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "nb = NaiveBayes()\n",
    "\n",
    "# Trasformazione label da String a Integer\n",
    "data_df = df_TweetLabeled_toFit.withColumn(\"label\", df_TweetLabeled[\"label\"].cast(IntegerType()))\n",
    "\n",
    "# Dichiarazione della pipeline\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, nb])\n",
    "model = pipeline.fit(data_df)\n",
    "\n",
    "# Valutazione del modello con dati di training\n",
    "predictions = model.transform(data_df)\n",
    "\n",
    "predictions.show(5)\n",
    "\n",
    "# Calcolo dell'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[words_nsw: array<string>, label: string]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_TweetLabeled_toFit.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
