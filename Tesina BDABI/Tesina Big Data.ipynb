{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tesina di Big Data and Business Intelligence - Sentiment Analysis</h1>\n",
    "<h3>Autori: Emanuele Di Fiore, Roberto Di Luca</h3>\n",
    "\n",
    "<div style=\"text-align: justify\"> \n",
    "<br>Nella presente tesina è stata svolta una Sentiment Analysis su un dataset di Tweet estratti dal social network Twitter per valutare le reazioni degli utenti durante il recente lockdown causato dalla diffusione del virus Covid-19. Le tecnologie utilizzate sono state:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apache Spark 2.4.5 (tramite la libreria pyspark) come ambiente per il preprocessing dei tweet, per la loro etichettatura, per l'addestramento e la valutazione di un modello di Machine Learning (ML);\n",
    "* MongoDB come database NoSQL per lo storage dei tweet;\n",
    "* Python per l'estrazione dei tweet tramite il tool Twarc.\n",
    "\n",
    "Il lavoro è strutturato come segue:\n",
    "* [Cenni alla teoria della Sentiment Analysis, estrazione dei tweet e introduzione al loro formato](#1)\n",
    "* [Etichettatura del dataset usando la libreria nltk (modello Vader)](#2);\n",
    "* [Addestramento di un modello di ML sul dataset etichettato e calcolo dell'accuracy](#3);\n",
    "* [SentiWordNet per l'etichettatura del dataset](#4);\n",
    "* [Analisi delle performance di Spark](#5).\n",
    "\n",
    "Inoltre è presente un'[appendice](#7) riguardante l'installazione in locale di Spark e MongoDB.\n",
    "<br>Infine, una sezione per i [riferimenti](#8) utilizzati nella trattazione.\n",
    "\n",
    "Tutto il codice sviluppato è presente su Github al seguente [link](https://github.com/emdifiore22/Covid19-Twitter-SentimentAnalysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'>\n",
    "    <h3>Sentiment Analysis e formato dei tweet</h3>\n",
    "</a>\n",
    "<div style=\"text-align: justify\">\n",
    "<br>Ogni giorno, grazie ai social network, ai blog o ad altri sistemi di raccomandazione, vengono scambiati milioni di messaggi su Internet. Tali messaggi possono essere suddivisi in due principali categorie: fatti e opinioni. I fatti sono affermazioni oggettive, mentre le opinioni riflettono un sentimento di un utente rispetto a un utente, altre persone o eventi e sono molto importanti quanche c'è la necessità di prendere delle decisioni. L'espressione <b>Sentiment Analysis</b> (anche nota come Opinion Mining) fa riferimento all'uso di tecniche di Natural Language Processing (NLP), Text Analysis e Linguistica Computazionale per identificare ed estrarre informazioni soggettive in documenti, commenti e post [1].\n",
    "<br>Un esempio di applicazione si ha nelle aziende che forniscono prodotti o servizi e che quindi sono interessate a conoscere i commenti o le opinioni dei loro clienti per garantire loro una qualità sempre più elevata.\n",
    "<br>Durante il recente periodo di lockdown che abbiamo vissuto a causa del virus Covid-19, molte persone hanno usato i social network, Twitter in particolare, per esprimere i loro stati d'animo. In questa tesina, è stata usata una parte del dataset GeoCoV19 [2]. I tweet scaricati sono stati memorizzati su MongoDB e preprocessati e analizzati tramite Spark al fine di conoscere quali sono stati i sentimenti predominanti.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Perché MongoDB?</h5>\n",
    "<div style=\"text-align: justify\">\n",
    "<br>Le tecnologie NoSQL sono particolarmente note per la loro caratteristica di essere \"schema-less\", ovvero i dati in essi memorizzati non devono necessariamente sottostare a uno schema prefissato così come nelle soluzioni relazionali. Tale caratteristica si presta molto bene al caso in esame, in cui i singoli tweet (la cui struttura è spiegata nel seguito) non rispettano rigorosamente una struttura. Ad esempio, se un tweet è originale (ovvero non ne ricondivide un altro) non presenta alcuni campi, tra cui il campo \"retweeted_status\".\n",
    "<br> In particolare, tra le varie soluzioni non relazionali, è stato scelto <b>MongoDB</b> per il suo orientamento ai documenti. Infatti, memorizza i dati in un formato JSON-like (BSON, Binary JSON), che è un \"cugino\" del formato JSON restituito dal tool utilizzato per estrarre i tweet. Questo ha reso molto rapida l'importazione dei dati nel DB.\n",
    "<br> Nella scelta di un DB NoSQL pesa anche il tipo di operazioni che si intende effettuare sui dati. Nel nostro caso, non è stato necessario analizzare eventuali relazioni tra i tweet, ma solo dei filtraggi sulla base dei valori per alcuni campi. Ciò ci ha portato a dire che una soluzione diversa da quella documentale, ad esempio una orientata ai grafi (Neo4j), non è la più indicata.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Estrazione dei Tweet</h5>\n",
    "<br>Sebbene i termini d'uso delle API di Twitter sconsiglino la condivisione via web dei dati raccolti, consentono quella di file contenenti gli id dei tweet. A questo punto, con un processo noto come \"Hydration\" è possibile ricavare l'intera struttura dati del tweet tramite il suo id.\n",
    "<br>Il processo di Hydration è molto semplice: data una collezione di identificativi (id) è possibile utilizzare <b>Twarc</b>, un tool, disponibile anche come libreria Python, che permette di scaricare tweet rappresentati in formato JSON. Per poter effettuare il download dei tweet è necessario creare un’app con un account developer di Twitter, ottenere le chiavi Consumer API e i token di accesso e utilizzare tali informazioni per abilitare Twarc al download. In particolare, tale abilitazione può avvenire in due modi:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Utilizzando il tool a linea di comando: lanciare twarc configure e seguire tutte le indicazioni per la registrazione delle chiavi e dei token\n",
    "* In Python, utilizzare il costruttore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "<br>Per poter estrarre i tweet è stato utilizzato lo script TwarcTwitterExtraction.py, disponibile nel repository GitHub del progetto. Tale script utilizza in input una lista di identificativi e genera in output un file in formato JSON Lines contenente i JSON dei tweet. Per tale elaborato sono stati gli identificativi associati alle date:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 13/02/2020\n",
    "* 16/03/2020\n",
    "* 29/03/2020\n",
    "* 30/03/2020\n",
    "* 08/04/2020\n",
    "* 29/04/2020\n",
    "\n",
    "A causa dell’elevata quantità di id presenti in questi file, essi sono stati processati soltanto in parte, arrivando a raccogliere 113965 tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Struttura di un tweet</h5>\n",
    "<br>In generale, un utente può postare un tweet in due modi:\n",
    "\n",
    "* Scrivendo un contenuto originale;\n",
    "* Condividendo un tweet di un altro utente.\n",
    "\n",
    "Dalla struttura JSON di un tweet è quindi possibile capirne la tipologia, il testo associato, informazioni sull’utente che l’ha postato, la data, le informazioni di geolocalizzazione, etc. Per lo svolgimento dell’elaborato sono stati fondamentali i seguenti campi o strutture:\n",
    "\n",
    "* <b>full_text</b>: contenente il testo completo del tweet;\n",
    "* <b>lang</b>: contenente la lingua con cui è stato scritto quel tweet;\n",
    "* <b>retweeted_status</b>: in caso di retweet, rappresenta una struttura innestata contenente tutte le informazioni del tweet che è stato ricondiviso (autore, testo del post, etc.), altrimenti il campo non è presente.\n",
    "\n",
    "Di seguito è mostrata la struttura completa di un tweet con e senza retweeted_status.\n",
    "![JSON dei tweet](images/tweet_json.jpg)\n",
    "\n",
    "<h5>Filtraggio</h5>\n",
    "<br>Una volta memorizzati tutti i tweet in una collection MongoDB, si è deciso di utilizzare per la Sentiment Analysis tutti i tweet in lingua inglese (campo lang uguale a ‘en’) e di selezionare il campo full_text per i tweet originali ed il campo retweeted_status.full_text per i tweet frutto di ricondivisione. Tale filtraggio si può facilmente effettuare tramite delle query ad hoc sul database. Le possibilità sono due:\n",
    "\n",
    "* Utilizzare il tool <b>MongoDB Compass</b> per effettuare le query utilizzando l’interfaccia grafica e poi esportare i risultati in formato JSON (da caricare successivamente in ambiente Spark);\n",
    "* Utilizzare direttamente <b>MongoDB Spark Connector</b>.\n",
    "\n",
    "Nel primo caso abbiamo:\n",
    "![MongoDB Compass](./images/query_no_retweet.jpg)\n",
    "![MongoDB Compass](./images/query_retweet.jpg)\n",
    "\n",
    "Nel secondo caso abbiamo il codice seguente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_noRetweet = \"[\\\n",
    "    {\\\n",
    "        '$match': {\\\n",
    "            'lang': 'en',\\\n",
    "            'retweeted_status':null\\\n",
    "        }\\\n",
    "    },{\\\n",
    "        '$project': {\\\n",
    "            'id_str': 1\\\n",
    "            'created_at': 1\\\n",
    "            'full_text': 1\\\n",
    "        },\\\n",
    "    }\\\n",
    "]\"\n",
    "\n",
    "pipeline_Retweet = \"[\\\n",
    "    {\\\n",
    "        '$match': {\\\n",
    "            'lang': 'en'\\\n",
    "            'retweeted_status':{$ne: null}\\\n",
    "            'retweeted_status.full_text':'en'\\\n",
    "        }\\\n",
    "    },{\\\n",
    "        '$project': {\\\n",
    "            'id_str': 1\\\n",
    "            'created_at': 1\\\n",
    "            'retweeted_status.full_text': 1\\\n",
    "        },\\\n",
    "    }\\\n",
    "]\"\n",
    "\n",
    "df_ENGNoRetweet = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline_noRetweet).load()\n",
    "df_ENGRetweet = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"pipeline\", pipeline_Retweet).load()\n",
    "\n",
    "df_ENGNoRetweet.printSchema()\n",
    "df_ENGRetweet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "In entrambi i casi l’idea è stata quella di selezionare, in una prima query, tutti i tweet che non presentano il campo retweeted_status e ricavare da essi il campo full_text, mentre in una seconda query il campo full_text è ricavato solo da quei tweet che presentano il campo retweeted_status. \n",
    "<br>In particolare, nel secondo caso (tweet frutto di ricondivisione) il campo full_text è identico al campo retweeted_status.full_text a meno dell’aggiunta di alcuni caratteri. Facciamo un esempio per chiarire la situazione. Supponiamo che Bob abbia ricondiviso il tweet “Spark è potentissimo!” di Alice, il tweet di Bob presenta nella propria struttura JSON:\n",
    "</div>\n",
    "\n",
    "* <b>full_text</b>: “RT @alice: Spark è potentissimo!”\n",
    "* <b>retweeted_status.full_text</b>: “Spark è potentissimo!”\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "Come si vede, il contenuto utile del tweet è lo stesso, si è deciso di considerare solo il secondo.\n",
    "Poiché, in generale, può capitare che più utenti ricondividano lo stesso tweet, potrebbero esserci dei testi duplicati. Per rimuoverli è stato utilizzata una distinct() sul DataFrame contenente i tweet.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Tweets = df_ENGRetweet\\\n",
    "    .selectExpr(\"id_str\", \"retweeted_status.full_text as full_text\")\\\n",
    "    .union(df_ENGNoRetweet.select(\"id_str\", \"full_text\"))\n",
    "\n",
    "\n",
    "df_Tweets = df_Tweets.select(\"full_text\").distinct()\n",
    "df_Tweets.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A valle del filtraggio, i tweet selezionati per la Sentiment Analysis sono 43710."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'>\n",
    "    <h3>Etichettatura tramite VADER</h3>\n",
    "</a>\n",
    "<div style=\"text-align: justify\">\n",
    "<br>Uno dei problemi principali nell'addestramento di modelli di ML per la sentiment analysis è la disponibilità di un dataset etichettato. Oltre ad essere un task time-consuming, l'etichettatura di un dataset del genere può essere anche complicata. Mentre per alcuni tweet è semplice estrarre la polarità del sentimento espresso (positiva, negativa, neutra), per altri può essere estremamente soggettiva. Di norma (e questo a prescindere dalla sentiment analysis), il labeling di un dataset è a cura di chi ha una profonda esperienza nel dominio che si sta trattando. Nella sentiment analysis, per quanto sopra riportato, questo è ancora più vero.\n",
    "In questa tesina, assumiamo come metodo di etichettatura quello effettuato tramite un modello preaddestrato chiamato VADER. \n",
    "<br><b>VADER (Valence Aware Dictionary and sEntiment Reasoner) [3] </b> è un tool di Sentiment Analysis di tipo rule-based specificamente progettato per i sentimenti espressi nei social media. VADER usa un lessico semantico, ovvero una lista di parole etichettate in base al loro orientamento ad essere positive o negative.\n",
    "<br>È stato osservato che VADER è molto performante quando si tratta di analizzare testi provenienti da social media, recensioni di film e di prodotti. Questo perché VADER non solo tratta i termini usuali del dizionario, ma anche espressioni tipiche del mondo della messaggistica istantanea come:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* uso delle contrazioni linguistiche (ad es. \"wasn't very good\");\n",
    "* uso della punteggiatura per accentuare l'intensità di un sentimento (ad es. \"Good!!!\");\n",
    "* uso della forma delle parole per conferirle maggiore enfasi (ad es. le parole scritte in maiuscolo);\n",
    "* uso delle emoticon.\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "L'uso di VADER è estremamente semplice in python grazie al pacchetto nltk (Natural Language ToolKit). Di seguito è mostrata una porzione di codice che mostra come avviene l'etichettatura. Il metodo polarity_scores() permette di ricavare gli indici di polarità (positivo, negativo, neutro) per una determinata frase.\n",
    "I positive, negative e neutral scores rappresentano la porzione di testo che ricade in tali categorie. Il compound score (valore compreso tra -1 e 1), invece, riassume in un unico valore la positività o la negatività di un testo. Se 1, il testo è totalmente positivo, -1 altrimenti.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "def vaderSentimentAnalysis(data_str):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    ss = sid.polarity_scores(data_str)\n",
    "    return ss\n",
    "\n",
    "vaderSentimentAnalysis_udf = udf(vaderSentimentAnalysis, StringType())\n",
    "\n",
    "df_Tweets = df_Tweets.withColumn(\"score\", vaderSentimentAnalysis_udf(df_Tweets['full_text']))\n",
    "df_Tweets.show(truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'>\n",
    "    <h3>Machine Learning</h3>\n",
    "</a>\n",
    "<div style=\"text-align: justify\">\n",
    "<br>In questa sezione, a partire dal dataset etichettato usando VADER, abbiamo addestrato un modello di ML di tipo <b>Naive Bayes</b>. La scelta di questo modello è motivata dal fatto che, sperimentalmente, tale genere di modello funziona particolarmente bene per scopi di Text Classification. Nonostante la sua semplicità, diversi sono i vantaggi derivanti dal suo uso: assenza di iperparametri da ottimizzare e velocità di addestramento rispetto ad altri modelli più complessi. \n",
    "<br>Di seguito è riportata la suddivisione del dataset (già opportunamente \"ripulito\" e suddiviso in token) in training e test set. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisione Training e Test\n",
    "train, test = df_TweetsCleaned.select(\"words_nsw\", \"label\").randomSplit([0.75,0.25], seed=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\">\n",
    "Le features estratte sono le classiche <b>TF-IDF (Term Frequency - Inverse Document Frequency)</b> [5].\n",
    "<br> La <b>term frequency (TF)</b> di una parola è la frequenza di occorrenza di una parola in un documento (nel nostro caso un tweet). Ad esempio, se un documento D di 100 parole contiene la parola \"cat\" 12 volte, allora la TF della parola \"cat\" è 12/100 = 0.12.\n",
    "<br> La <b>inverse document frequency (IDF)</b> di una parola è una misura che rispecchia l'importanza di un termine in una collezione documentale (nel nostro caso l'insieme dei tweet). Ad esempio, supponendo che una collezione documentale sia composta da 10 milioni di documenti e che la parola \"cat\" compaia solo in 300 mila documenti, la sua IDF è data dal log(10,000,000/300,000) = 1.52.\n",
    "<br> In conclusione, la parola \"cat\" ha una TF-IDF per il documento D pari a TF*IDF = 0.12*1.52 = 1.82.\n",
    "<br>In ML si è soliti eseguire una sequenza di algoritmi per processare e apprendere dai dati. Spark viene in aiuto all'esigenza di definire tali workflow fornendo l'astrazione di <b>Pipeline</b>. Una Pipeline consiste in una sequenza di stages (<b>Transformers</b>, che hanno come output dei DataFrame, ed <b>Estimators</b>, che hanno come output dei Transformers).\n",
    "Nel codice riportato è stata definita una pipeline che, in sequenza, crea TF, IDF, e il modello Bayesiano. Infine è stato calcolato sia il resubstitution error, sia quello sul test set.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv  = CountVectorizer(inputCol='words_nsw', outputCol='tf')\n",
    "idf = IDF().setInputCol('tf').setOutputCol('features')\n",
    "nb  = NaiveBayes()\n",
    "\n",
    "pipeline = Pipeline(stages=[cv, idf, nb])\n",
    "\n",
    "# Dichiarazione della pipeline\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Valutazione del modello con dati di training\n",
    "predictions_train = model.transform(train)\n",
    "\n",
    "# Calcolo dell'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "eval_train = evaluator.evaluate(predictions_train)\n",
    "\n",
    "# Valutazione del modello con dati di test\n",
    "predictions_test = model.transform(test)\n",
    "\n",
    "# Calcolo dell'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "eval_test = evaluator.evaluate(predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'>\n",
    "    <h3>SentiWordNet</h3>\n",
    "</a>\n",
    "<div style=\"text-align: justify\">\n",
    "<br><b>SENTIWORDNET</b> [4] è una risorsa lessicale estremamente utile in materia di Sentiment Analysis. SENTIWORDNET è il risultato dell'annotazione di ogni synset di WORDNET in accordo alla nozione di \"positività\", \"negatività\" e \"neutralità\". Un <b>synset (synonym set)</b> è un insieme di sinonimi che possono essere descritti da un'unica definizione, perché esprimono uno stesso senso. Una medesima parola, quindi, si può trovare in diversi synset se ha diversi sensi (significati). \n",
    "<br>Nell'ambito di SENTIWORDNET, a ogni synset sono associati tre scores, <b>Pos(s)</b>, <b>Neg(s)</b> e <b>Obj(s)</b>, che indicano quanto sono positivi, negativi e oggettivi (cioè neutri) i termini contenuti nel synset. Ciascuno dei tre scores varia nell'intervallo [0, 1] e la loro somma è unitaria per ogni synset.\n",
    "<br>Ad esempio, il synset <b>[estimable(J,3)]</b> (J nel gergo di WORDNET sta per \"aggettivo\"), corrispondente al senso \"may be computed or estimated\" dell'aggettivo \"estimable\" ha un Obj score pari a 1, mentre Pos e Neg score pari a 0. Al contrario, il synset <b>[estimable(J,1)]</b>, corrispondente al senso “deserving of respect or high regard” ha un Pos score pari a 0.75, un Neg score nullo e un Obj score di 0.25.\n",
    "</div>\n",
    "\n",
    "L'approccio seguito in questo per assegnare un sentimento a ciascun tweet del dataset è composto dai seguenti passi:\n",
    "* <b>rimozione dei caratteri speciali</b> dalla stringa corrispondente al tweet;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "def remove_all_space(astring):\n",
    "  return \" \".join(astring.split())\n",
    "\n",
    "# clean the text \n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    cleaned_str2 = remove_all_space(cleaned_str)\n",
    "    return cleaned_str2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>tokenizzazione</b> e <b>rimozione delle stopwords</b>;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizzazione\n",
    "tkn = Tokenizer()\\\n",
    "      .setInputCol(\"cleaned_text\")\\\n",
    "      .setOutputCol(\"words\")\n",
    "\n",
    "# Eliminazione Stopwords\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "        .setStopWords(englishStopWords)\\\n",
    "        .setInputCol(\"words\")\\\n",
    "        .setOutputCol(\"words_nsw\")\n",
    "\n",
    "pipeline = Pipeline(stages = [tkn, stops])\n",
    "\n",
    "df_TweetsCleaned = pipeline\\\n",
    "    .fit(df_TweetsCleaned.select(\"full_text\", \"cleaned_text\"))\\\n",
    "    .transform(df_TweetsCleaned.select(\"full_text\", \"cleaned_text\"))\n",
    "\n",
    "df_TweetsCleaned.select(\"full_text\", \"cleaned_text\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>estrazione del tag dai token</b>. Con questo si intende effettuare una sorta di analisi grammaticale del testo per capire se un termine è un nome, un aggettivo, un verbo o un avverbio. Questa informazione è utile sia per la corretta lemmatizzazione (vedi punto successivo), sia per estrarre il corretto synset da SENTIWORDNET. Il metodo pos_tag del pacchetto nltk fa proprio al caso nostro. Ad esempio, se abbiamo una lista ('play','cards'), il risultato sarà (('play','v'), ('cards','n'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract part of speech\n",
    "def pos(tokenized_text):\n",
    "    sent_tag_list = pos_tag(tokenized_text) \n",
    "    aList = []\n",
    "    for word, tag in sent_tag_list:\n",
    "        tagToUse = ''\n",
    "        if tag.startswith('J'):\n",
    "            tagToUse= 'a' # aggettivi\n",
    "        elif tag.startswith('N'):\n",
    "            tagToUse= 'n' # sostantivi\n",
    "        elif tag.startswith('R'):\n",
    "            tagToUse= 'r' # avverbi\n",
    "        elif tag.startswith('V'):\n",
    "            tagToUse= 'v' # verbi\n",
    "        else:\n",
    "            continue\n",
    "        aList.append((word, tagToUse))\n",
    "    return aList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>lemmatizzazione</b>, ovvero il processo di conversione di una parola nella sua forma base. La differenza con lo stemming è che questo rimuove solo gli ultimi caratteri di una parola, portando spesso a una forma sbagliata, mentre la lemmatizzazione considera il contesto della parola, estraendo quindi il corretto significato;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize the tokens \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(array_of_word_for_a_comment):\n",
    "    all_words_in_comment = []\n",
    "    for word in array_of_word_for_a_comment:\n",
    "        lemma = lemmatizer.lemmatize(word[0], pos=word[1])\n",
    "        if not lemma:\n",
    "            continue\n",
    "        all_words_in_comment.append([lemma,word[1]])  \n",
    "    return all_words_in_comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <b>calcolo dello score</b> di una frase. Lo score di un tweet è dato dalla somma dei contributi (in termini di positive, negative e neutral score) di tutti i token contenuti al suo interno e in SENTIWORDNET;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the sentiment \n",
    "def cal_score(array_of_lemma_tag_for_a_comment):\n",
    "    alist = [array_of_lemma_tag_for_a_comment]\n",
    "    totalScore = 0\n",
    "    count_words_included = 0\n",
    "    for word in array_of_lemma_tag_for_a_comment:\n",
    "        synset_forms = list(swn.senti_synsets(word[0], word[1]))\n",
    "        if not synset_forms:\n",
    "            continue\n",
    "        synset = synset_forms[0] \n",
    "        totalScore = totalScore + synset.pos_score() - synset.neg_score()\n",
    "        count_words_included = count_words_included +1\n",
    "    final_dec = ''\n",
    "    if count_words_included == 0:\n",
    "        final_dec = 'N/A'\n",
    "    elif totalScore == 0:\n",
    "        final_dec = 'Neu'        \n",
    "    elif totalScore/count_words_included < 0:\n",
    "        final_dec = 'Neg'\n",
    "    elif totalScore/count_words_included > 0:\n",
    "        final_dec = 'Pos'\n",
    "    return final_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Tutte le funzioni definite sono poi state registrate come udf (User Defined Function) in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features_udf = udf(remove_features, StringType())\n",
    "pos_udf = udf(pos,ArrayType(StructType([ StructField(\"word\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "lemmatize_udf = udf(lemmatize,ArrayType(StructType([ StructField(\"lemma\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "cal_score_udf = udf(cal_score,StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5'>\n",
    "    <h3>Analisi delle performance di Spark</h3>\n",
    "</a>\n",
    "<div style=\"text-align: justify\">\n",
    "<br> In questa sezione riportiamo alcune considerazioni riguardanti le performance di Spark. Sebbene sia stata utilizzata una macchina virtuale e i tempi siano stati misurati sfruttando la direttiva time fornita da Jupyter (che, per la precisione, misura il cosiddetto wall time, ovvero non solo il tempo di utilizzo della CPU del singolo processo, ma anche quello dovuto all'interferenza di altri processi concorrenti), è comunque evidente il vantaggio nell'utilizzo di questo framework. Per valutare le performance, sono state confrontate due implementazioni dell'etichettatura tramite il modello VADER: una in pyspark (riportata sopra) e una in Python senza Spark. In particolare, per quanto riguarda la seconda, ne sono state preparate tre versioni: una prima che non sfrutta meccanismi di parallelizzazione del codice (usando libreria pandas); una seconda analoga alla prima ma che non usa pandas; una terza basata sulla libreria Multiprocessing per trarre beneficio dei più core sulla macchina per l'esecuzione delle funzioni sul DataFrame Pandas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Esecuzioni</h5>\n",
    "<div style=\"text-align: justify\">\n",
    "\n",
    "<br>Abbiamo eseguito l'algoritmo in ambiente Spark, usando il contesto di default creato all'avvio del notebook Jupyter. In tale situazione, Spark crea tanti worker (threads) quanti sono i core logici sulla macchina (local[*]). In questo caso, il tempo di esecuzione su 43710 tweet è pari circa a 18s.\n",
    "Il passo successivo è stato quello di eseguire le due versioni senza Spark. I risultati sono riportati di seguito:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python Pandas = 772s\n",
    "* Python puro = 719s\n",
    "\n",
    "<div style=\"text-align: justify\">\n",
    "All'atto della creazione del DataFrame, Spark suddivide i dati su più partizioni (di default 200) in modo tale che tutti i worker lavorino sulla propria partizione in parallelo. Il programmatore ha due possibilità: la prima consiste nell'ignorare aspetti relativi alla concorrenza e alla configurazione della macchina (o del cluster) su cui esegue il codice; la seconda consiste nella scelta del numero di worker e del numero di partizioni in cui suddividere il dataframe. Nel secondo caso, è possibile lanciare pyspark con l'opzione --master local[K], dove K è proprio il numero di worker thread desiderato (idealmente pari al numero di core della macchina) e, nel codice, tramite oppurtune funzioni (parallelize(), repartition(), ...) controllare il numero di partizioni.\n",
    "Poiché la macchina virtuale gira su due core fisici, abbiamo lanciato pyspark specificando 2 worker e suddiviso il dataframe in 2 partizioni. Il tempo d'esecuzione è stato di 345s.\n",
    "Allo stesso modo, abbiamo lanciato lo script python con la libreria Multiprocessing indicando 2 come grado parallelizzazione. Il risultato ottenuto è stato di 424s.\n",
    "<br>Infine, i tempi relativi all'esecuzione su Spark potrebbero anche coinvolgere operazioni di lettura da MongoDB per la sua caratteristica di lazy evaluation. Invece, i tempi relativi alle esecuzioni \"no Spark\" si riferiscono esclusivamente all'etichettatura.\n",
    "\n",
    "Chiaramente, questa analisi non è stata svolta in modo accurato, ma nonostante ciò è evidente che l'astrazione dei DataFrame Spark nasconda meccanismi per parallelizzare l'esecuzione delle istruzioni in modo trasparente al programmatore, rendendolo più efficiente rispetto alle altre soluzioni provate.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'>\n",
    "    <h3>Installazione Spark e MongoDB</h3>\n",
    "</a>\n",
    "\n",
    "<h5>Installazione Apache Spark</h5>\n",
    "<br>Per quanto riguarda l’installazione di Apache Spark in locale, una possibilità è quella di utilizzare una virtual machine, in modo da isolare completamente Spark dal sistema operativo host, mantenendo comunque la possibilità di eseguire script PySpark su dei notebook Jupyter. \n",
    "Installata una macchina virtuale Linux (ad esempio XUbuntu), è possibile installare correttamente Spark seguendo le indicazioni qui riportate:<br>\n",
    "\n",
    "* Scaricare l’ultima versione di Spark dal sito ufficiale.\n",
    "* Creare la cartella spark  in <span style=\"background: rgb(255, 0, 87); border-radius: 5px !important; color: white; padding: 3px\">/usr/lib/</span> contenente tutti i file di Spark;\n",
    "* Installare SBT;\n",
    "* Installare Java 8;\n",
    "* Configurare Spark attraverso il file <span style=\"background: rgb(255, 0, 87); border-radius: 5px !important; color: white; padding: 3px\">/usr/lib/spark/conf/spark-env.sh</span> (eventualmente generarlo med iante template presente nella stessa cartella), aggiungendo le righe:\n",
    "    * <span style=\"font-family: 'Andale Mono'\">JAVA_HOME=/usr/lib/jvm/java-8-oracle</span>\n",
    "    * <span style=\"font-family: 'Andale Mono'\">SPARK_WORKER_MEMORY=4g</span>\n",
    "* Installare Anaconda 3;</span>\n",
    "* Modificare le variabili d’ambiente nel file ~/.bashrc :\n",
    "    * <span style=\"font-family: 'Andale Mono'\">export JAVA_HOME=/usr/lib/jvm/java-8-oracle </span> \n",
    "    * <span style=\"font-family: 'Andale Mono'\">export SBT_HOME=/usr/share/sbt-launcher-packaging/bin/sbt-launch.jar  </span>\n",
    "    * <span style=\"font-family: 'Andale Mono'\">export SPARK_HOME=/usr/lib/spark</span>\n",
    "    * <span style=\"font-family: 'Andale Mono'\">export PATH=<span>&#36;</span>PATH:<span>&#36;</span>JAVA_HOME/bin</span>\n",
    "    * <span style=\"font-family: 'Andale Mono'\">export PATH=<span>&#36;</span>PATH:<span>&#36;</span>SBT_HOME/bin:<span>&#36;</span>SPARK_HOME/bin:<span>&#36;</span>SPARK_HOME/sbin</span>\n",
    "    * <span style=\"font-family: 'Andale Mono'\">export PYSPARK_DRIVER_PYTHON=jupyter</span>\n",
    "    * <span style=\"font-family: 'Andale Mono'\">export PYSPARK_DRIVER_PYTHON_OPTS='notebook'</span>\n",
    "    * <span style=\"font-family: 'Andale Mono'\">export PYSPARK_PYTHON=python2.7</span>\n",
    "    * <span style=\"font-family: 'Andale Mono'\">export PYTHONPATH=<span>&#36;</span>SPARK_HOME/python:<span>&#36;</span>PYTHONPATH</span>\n",
    "* Verificare la corretta installazione lanciando il commando pyspark sul terminale e visualizzato il notebook Jupyter a localhost:8888 (aperto automaticamente).\n",
    "\n",
    "La lista dettagliata dei comandi da utilizzare è presente nella guida [1]. \n",
    "In definitiva, la configurazione utilizzata per l’elaborato è dunque la seguente:\n",
    "<br>\n",
    "\n",
    "* Spark 2.4.5 - Hadoop2.7\n",
    "* XUbuntu 20.04 - 4 GB di RAM, 2 CPU\n",
    "* Anaconda 3 per i packages necessari\n",
    "\n",
    "<h5>Installazione MongoDB</h5>\n",
    "<br>Il database NoSQL MongoDB è stato installato sulla macchina host. Per l'installazione è stato necessario scaricare i file sorgenti dal sito ufficiale e seguire documentazione [2]. \n",
    "In particolare è stata utilizzata la versione 2.4.2 Community Server scaricabile a [3]. Scaricato il necessario, per lanciare il server è sufficiente spostarsi nella cartella <span <span style=\"background: rgb(255, 0, 87); border-radius: 5px !important; color: white; padding: 3px\">/mongodb/bin/</span> e lanciare il comando\n",
    "\n",
    "* <span style=\"font-family: 'Andale Mono'\">./mongod --bind_ip localhost,[IP ADDRESS] --dbpath [Path-to-dbFolder]</span>\n",
    "\n",
    "<h5>Connessione Spark-MongoDB</h5>\n",
    "<br>Avendo installato Spark in locale, è possibile dunque caricare direttamente dal database i file necessari per le elaborazioni in PySpark su notebook Jupyter, utilizzando il MongoDB Spark Connector [4]. Per installare il connettore è sufficiente copiare nella cartella <span style=\"background: rgb(255, 0, 87); border-radius: 5px !important; color: white; padding: 3px\">/usr/lib/spark/jars/</span> i seguenti file (disponibili nel repository GitHub):<br>\n",
    "\n",
    "* bson-3.8.1.jar\n",
    "* mongodb-driver-core-3.8.1.jar\n",
    "* mongodb-driver-3.8.1.jar\n",
    "* mongo-spark-connector_2.11-2.4.2.jar\n",
    "\n",
    "Una volta caricati i jar necessari, per poter caricare un DataFrame con il contenuto di una collection presente su MongoDB è necessario utilizzare una SparkSession:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://[MongoDB IP ADDRESS]/[DatabaseName].[CollectionName]?retryWrites=true\")\\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://[MongoDB IP ADDRESS]/[DatabaseName].[CollectionName]?retryWrites=true\")\\\n",
    "    .getOrCreate()\n",
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8'>\n",
    "    <h3>Riferimenti</h3>\n",
    "</a>\n",
    "\n",
    "[1] Probabilistic Approaches for Sentiment Analysis: Latent Dirichlet Allocation for Ontology Building and Sentiment Extraction, Colace F., De Santo M.\n",
    "\n",
    "[2] GeoCoV19: A Dataset of Hundreds of Millions of Multilingual COVID-19 Tweets with Location Information\n",
    "\n",
    "[3] VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text\n",
    "\n",
    "[4] SENTIWORDNET 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining\n",
    "\n",
    "[5] What is TF-IDF: https://www.onely.com/blog/what-is-tf-idf/\n",
    "\n",
    "<h5>Riferimenti per l'installazione</h5>\n",
    "\n",
    "[6]https://medium.com/@brajendragouda/installing-apache-spark-on-ubuntu-pyspark-on-juputer-ca8e40e8e655\n",
    "\n",
    "[7]https://docs.mongodb.com/manual/tutorial/install-mongodb-on-windows/\n",
    "\n",
    "[8]https://www.mongodb.com/try/download/community\n",
    "\n",
    "[9]https://docs.mongodb.com/spark-connector/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
