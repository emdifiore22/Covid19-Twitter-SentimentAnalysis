{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import string, re, json\n",
    "\n",
    "# Import NLTK\n",
    "import nltk\n",
    "import sys\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/emanuele/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download the needed corpus \n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGRetweet = \"./ENGRetweet.json\"\n",
    "ENGNoRetweet = \"./ENGNoRetweet.json\"\n",
    "\n",
    "# Creazione dei dataFrame\n",
    "df_ENGRetweet = spark.read\\\n",
    "    .format(\"json\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"multiLine\", \"true\")\\\n",
    "    .load(ENGRetweet)\n",
    "\n",
    "df_ENGNoRetweet = spark.read\\\n",
    "    .format(\"json\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"multiLine\", \"true\")\\\n",
    "    .load(ENGNoRetweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1050"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNIONE DEI RISULTATI\n",
    "\n",
    "df_Tweets = df_ENGRetweet\\\n",
    "    .selectExpr(\"id_str\", \"retweeted_status.full_text as full_text\")\\\n",
    "    .union(df_ENGNoRetweet.select(\"id_str\", \"full_text\"))\n",
    "\n",
    "\n",
    "df_Tweets = df_Tweets.select(\"full_text\").distinct()\n",
    "df_Tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "def remove_all_space(astring):\n",
    "  return \" \".join(astring.split())\n",
    "\n",
    "# clean the text \n",
    "def remove_features(data_str):\n",
    "    # compile regex\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    num_re = re.compile('(\\\\d+)')\n",
    "    alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n",
    "    # convert to lowercase\n",
    "    data_str = data_str.lower()\n",
    "    # remove hyperlinks\n",
    "    data_str = url_re.sub(' ', data_str)\n",
    "    # remove puncuation\n",
    "    data_str = punc_re.sub(' ', data_str)\n",
    "    # remove numeric 'words'\n",
    "    data_str = num_re.sub(' ', data_str)\n",
    "    # remove non a-z 0-9 characters and words shorter than 3 characters\n",
    "    list_pos = 0\n",
    "    cleaned_str = ''\n",
    "    for word in data_str.split():\n",
    "        if list_pos == 0:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = word\n",
    "            else:\n",
    "                cleaned_str = ' '\n",
    "        else:\n",
    "            if alpha_num_re.match(word) and len(word) > 2:\n",
    "                cleaned_str = cleaned_str + ' ' + word\n",
    "            else:\n",
    "                cleaned_str += ' '\n",
    "        list_pos += 1\n",
    "    cleaned_str2 = remove_all_space(cleaned_str)\n",
    "    return cleaned_str2\n",
    "\n",
    "\n",
    "# extract part of speech\n",
    "def pos(tokenized_text):\n",
    "    sent_tag_list = pos_tag(tokenized_text) \n",
    "    aList = []\n",
    "    for word, tag in sent_tag_list:\n",
    "        tagToUse = ''\n",
    "        if tag.startswith('J'):\n",
    "            tagToUse= 'a' # aggettivi\n",
    "        elif tag.startswith('N'):\n",
    "            tagToUse= 'n' # sostantivi\n",
    "        elif tag.startswith('R'):\n",
    "            tagToUse= 'r' # avverbi\n",
    "        elif tag.startswith('V'):\n",
    "            tagToUse= 'v' # verbi\n",
    "        else:\n",
    "            continue\n",
    "        aList.append((word, tagToUse))\n",
    "    return aList\n",
    "\n",
    "# lemmatize the commit comments  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(array_of_word_for_a_comment):\n",
    "    all_words_in_comment = []\n",
    "    for word in array_of_word_for_a_comment:\n",
    "        lemma = lemmatizer.lemmatize(word[0], pos=word[1])\n",
    "        if not lemma:\n",
    "            continue\n",
    "        all_words_in_comment.append([lemma,word[1]])  \n",
    "    return all_words_in_comment\n",
    "\n",
    "\n",
    "#calculate the sentiment \n",
    "def cal_score(array_of_lemma_tag_for_a_comment):\n",
    "    alist = [array_of_lemma_tag_for_a_comment]\n",
    "    totalScore = 0\n",
    "    count_words_included = 0\n",
    "    for word in array_of_lemma_tag_for_a_comment:\n",
    "        synset_forms = list(swn.senti_synsets(word[0], word[1]))\n",
    "        if not synset_forms:\n",
    "            continue\n",
    "        synset = synset_forms[0] \n",
    "        totalScore = totalScore + synset.pos_score() - synset.neg_score()\n",
    "        count_words_included = count_words_included +1\n",
    "    final_dec = ''\n",
    "    if count_words_included == 0:\n",
    "        final_dec = 'N/A'\n",
    "    elif totalScore == 0:\n",
    "        final_dec = 'Neu'        \n",
    "    elif totalScore/count_words_included < 0:\n",
    "        final_dec = 'Neg'\n",
    "    elif totalScore/count_words_included > 0:\n",
    "        final_dec = 'Pos'\n",
    "    return final_dec\n",
    "\n",
    "\n",
    "remove_features_udf = udf(remove_features, StringType())\n",
    "pos_udf = udf(pos,ArrayType(StructType([ StructField(\"word\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "lemmatize_udf = udf(lemmatize,ArrayType(StructType([ StructField(\"lemma\", StringType(), False), StructField(\"tag\", StringType(), False)])))\n",
    "cal_score_udf = udf(cal_score,StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove noise\n",
    "df_TweetsCleaned = df_Tweets.withColumn(\"cleaned_text\", remove_features_udf(df_Tweets['full_text']))\n",
    "#df_TweetsCleaned.select('cleaned_text').show(truncate=50)\n",
    "df_TweetsCleaned.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           full_text|        cleaned_text|\n",
      "+--------------------+--------------------+\n",
      "|\"The United State...|the united states...|\n",
      "|Coronavirus &amp;...|coronavirus amp c...|\n",
      "|Oh to be a 1998 b...|baby first memory...|\n",
      "|Hookah should be ...|hookah should ban...|\n",
      "|@caitrionambalfe ...|caitrionambalfe n...|\n",
      "|tangled will be T...|tangled will the ...|\n",
      "|#Epeeps Please ta...|epeeps please tak...|\n",
      "|Chairman and CEO ...|chairman and ceo ...|\n",
      "|As an ER doc tryi...|doc trying treat ...|\n",
      "|Public News Servi...|public news servi...|\n",
      "|Wash your hands, ...|wash your hands s...|\n",
      "|Ohhhhh my frick, ...|ohhhhh frick than...|\n",
      "|@Colonel_Eevee Ur...|colonel eevee exc...|\n",
      "|@patmcguinness @n...|patmcguinness nev...|\n",
      "|Exclusive. Access...|exclusive access ...|\n",
      "|#coronavirus upda...|coronavirus updat...|\n",
      "|On question what ...|question what can...|\n",
      "|New: Starting imm...|new starting imme...|\n",
      "|The safest bet is...|the safest bet us...|\n",
      "|Me, 2 weeks ago: ...|weeks ago twitter...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizzazione\n",
    "tkn = Tokenizer()\\\n",
    "      .setInputCol(\"cleaned_text\")\\\n",
    "      .setOutputCol(\"words\")\n",
    "\n",
    "# Eliminazione Stopwords\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "        .setStopWords(englishStopWords)\\\n",
    "        .setInputCol(\"words\")\\\n",
    "        .setOutputCol(\"words_nsw\")\n",
    "\n",
    "pipeline = Pipeline(stages = [tkn, stops])\n",
    "\n",
    "\n",
    "df_TweetsCleaned = pipeline\\\n",
    "    .fit(df_TweetsCleaned.select(\"full_text\", \"cleaned_text\"))\\\n",
    "    .transform(df_TweetsCleaned.select(\"full_text\", \"cleaned_text\"))\n",
    "\n",
    "df_TweetsCleaned.select(\"full_text\", \"cleaned_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- words_nsw: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- words_nsw_tag: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- word: string (nullable = false)\n",
      " |    |    |-- tag: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#tag for part of speech\n",
    "df_TweetsCleanedTagged = df_TweetsCleaned.withColumn(\"words_nsw_tag\", pos_udf(df_TweetsCleaned['words_nsw']))\n",
    "#df_TweetsCleanedTagged.select(\"words_nsw_tag\").show(truncate=50)\n",
    "df_TweetsCleanedTagged.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- cleaned_text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- words_nsw: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- words_nsw_tag: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- word: string (nullable = false)\n",
      " |    |    |-- tag: string (nullable = false)\n",
      " |-- words_lemmatized: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- lemma: string (nullable = false)\n",
      " |    |    |-- tag: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lemmatize the tokens \n",
    "df_Tweet_Lemmatized = df_TweetsCleanedTagged.withColumn(\"words_lemmatized\", lemmatize_udf(df_TweetsCleanedTagged['words_nsw_tag']))\n",
    "#df_Tweet_Lemmatized.select(\"words_lemmatized\").show(truncate=50)\n",
    "df_Tweet_Lemmatized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----+\n",
      "|                                         full_text|score|\n",
      "+--------------------------------------------------+-----+\n",
      "|\"The United States, long accustomed to thinking...|  Pos|\n",
      "|Coronavirus &amp; climate change demand similar...|  Pos|\n",
      "|Oh to be a 1998 baby\n",
      "\n",
      "‚úîÔ∏è first memory is 9/11\n",
      "\n",
      "...|  Neg|\n",
      "|Hookah should be banned at groove and restauran...|  Neu|\n",
      "|@caitrionambalfe @netflix My husband and I have...|  Pos|\n",
      "|tangled will be THE movie of self-isolation #CO...|  Neu|\n",
      "|#Epeeps Please take care  ü¶†ü¶†ü¶†üî•üî•üî•  #COVID1...|  Neu|\n",
      "|Chairman and CEO of Universal Music Diagnosed W...|  Pos|\n",
      "|As an ER doc trying to treat patients who may h...|  Pos|\n",
      "|Public News Service Daily Newscast: Biden and S...|  Neg|\n",
      "|Wash your hands, self quarantine, and don‚Äôt pan...|  Pos|\n",
      "|Ohhhhh my frick, thank you!!! https://t.co/lQHB...|  Neu|\n",
      "|@Colonel_Eevee Ur excellent. Remember being sad...|  Pos|\n",
      "|@patmcguinness @nevancik1 @chicagosmayor @realD...|  Pos|\n",
      "|Exclusive. Access. \n",
      "\n",
      "Why though? https://t.co/5...|  Neg|\n",
      "|#coronavirus update in US. New cases confirmed ...|  Pos|\n",
      "|On question what candidates would do 2 help ree...|  Pos|\n",
      "|New: Starting immediately and for the next 8 we...|  Pos|\n",
      "|The safest bet is to use disinfectant wipes tha...|  Pos|\n",
      "|Me, 2 weeks ago: Twitter is breaking my brain, ...|  Neg|\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate the sentiment\n",
    "try:\n",
    "    df_Tweet_Lemmatized = df_Tweet_Lemmatized.withColumn(\"score\", cal_score_udf(df_Tweet_Lemmatized[\"words_lemmatized\"]))\n",
    "except:#\n",
    "    sys.setrecursionlimit(2000)\n",
    "    df_Tweet_Lemmatized = df_Tweet_Lemmatized.withColumn(\"score\", cal_score_udf(df_Tweet_Lemmatized[\"words_lemmatized\"]))\n",
    "\n",
    "df_Tweet_Lemmatized.select(\"full_text\",\"score\").show(truncate=50)\n",
    "#df_Tweet_Lemmatized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
