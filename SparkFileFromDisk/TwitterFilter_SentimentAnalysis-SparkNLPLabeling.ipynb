{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import string, re, json\n",
    "import sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento dati necessari\n",
    "#\n",
    "#\n",
    "ITRetweet = \"./ITRetweet.json\"\n",
    "ITNoRetweet = \"./ITNoRetweet.json\"\n",
    "\n",
    "# Creazione dataFrame\n",
    "df_ITRetweet = spark.read.format(\"json\").option(\"inferSchema\", \"true\").option(\"multiLine\", \"true\").load(ITRetweet)\n",
    "df_ITNoRetweet = spark.read.format(\"json\").option(\"inferSchema\", \"true\").option(\"multiLine\", \"true\").load(ITNoRetweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      " |-- retweeted_status: struct (nullable = true)\n",
      " |    |-- full_text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ITRetweet\n",
    "\n",
    "df_ITRetweet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- full_text: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- id_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ITNoRetweet\n",
    "\n",
    "df_ITNoRetweet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# UNIONE DEI RISULTATI\n",
    "\n",
    "df_Tweets = df_ITRetweet\\\n",
    "    .selectExpr(\"id_str\", \"retweeted_status.full_text as full_text\")\\\n",
    "    .union(df_ITNoRetweet.select(\"id_str\", \"full_text\"))\n",
    "\n",
    "\n",
    "df_Tweets = df_Tweets.select(\"full_text\").distinct()\n",
    "df_Tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione funzione per l'eliminazione dei caratteri speciali\n",
    "#\n",
    "#\n",
    "\n",
    "def remove_punct(text):\n",
    "    url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n",
    "    punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    mention_re = re.compile('@(\\w+)')\n",
    "    special = re.compile('[\\$#,@&%£!=°§*/;-]')\n",
    "    num_re = re.compile('( \\\\d+)')\n",
    "    text = url_re.sub(\"\", text)\n",
    "    text = punc_re.sub(\"\", text)\n",
    "    text = mention_re.sub(\"\", text)\n",
    "    text = special.sub(\"\", text)\n",
    "    text = num_re.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "# setup pyspark udf function\n",
    "remove_features_udf = udf(lambda x: remove_punct(x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_Tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7367db39f5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Eliminazione caratteri speciali\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_noHash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_Tweets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'words_filtered'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mremove_features_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"full_text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tokenizzazione\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtkn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_Tweets' is not defined"
     ]
    }
   ],
   "source": [
    "# Eliminazione caratteri speciali\n",
    "df_noHash = df_Tweets.withColumn('words_filtered',remove_features_udf(\"full_text\"))\n",
    "\n",
    "# Tokenizzazione\n",
    "tkn = Tokenizer()\\\n",
    "      .setInputCol(\"words_filtered\")\\\n",
    "      .setOutputCol(\"words\")\n",
    "\n",
    "# Eliminazione Stopwords\n",
    "italianStopWords = StopWordsRemover.loadDefaultStopWords(\"italian\")\n",
    "stops = StopWordsRemover()\\\n",
    "        .setStopWords(italianStopWords)\\\n",
    "        .setInputCol(\"words\")\\\n",
    "        .setOutputCol(\"words_nsw\")\n",
    "\n",
    "pipeline = Pipeline(stages = [tkn, stops])\n",
    "\n",
    "df_TweetCleaned = pipeline.fit(df_noHash.select(\"words_filtered\")).transform(df_noHash.select(\"words_filtered\"))\n",
    "\n",
    "df_TweetCleaned.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARK NLP #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
